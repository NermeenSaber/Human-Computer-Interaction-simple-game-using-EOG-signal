# -*- coding: utf-8 -*-
"""EOG_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r_4EDaYkZMwNhSMo49ZQs7Z9-xkeG9YE

# Import Libraries
"""
close_train=True
import os

from scipy import signal
from scipy.signal import butter, filtfilt

import numpy as np
import scipy.integrate as integrate
from pylab import figure, clf, plot, xlabel, ylabel, title, grid, axes, show
from scipy.signal import find_peaks

from sklearn import svm
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.utils import shuffle
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

"""# Data Preprocessing

Both horizontal and vertical signals are filtered by Butterworth band pass filter to preserve EOG band from 1 to 20-30 HZ.

**order**: in many studies order **2** is sufficient

In signal processing, a signal is typically represented as a continuous-time represintation so we need to convert it into a discrete-time representation. This process is called **sampling**.

The **sampling** **rate** is the number of samples per unit time.

**nyq:** The Nyquist frequency is half of the sampling rate,The Nyquist frequency is the highest frequency that can be represented by the digital signal without causing aliasing.
"""


def Butterworth_filter(signal, low_cutoff, high_cutoff, sampling_rate, order):
    nyq = 0.5 * sampling_rate
    low = low_cutoff / nyq
    high = high_cutoff / nyq
    numinator, demonator = butter(order, [low, high], btype='band', output='ba', analog=False, fs=None)
    #applying filter on signal
    after_filter = filtfilt(numinator, demonator, signal)
    return after_filter


'''
order        : the order of the filter
[low, high]  : the normalized frequency range of the bandpass filter
btype='band' : bandpass filter
output='ba'  : the output in numerator-denominator form
analog=False :a digital (not analog) filter
fs=None      : we are not working with an analog filter, so this parameter can be set to None

'''

"""Resampling step will produce a new signal with 128 samples,where #samples =250 in the original signal."""


def resampling_signal(signal_, num):
    return signal.resample(signal_, num)


"""we **normalize** the input signal to a range of 0 to 100, by subtracting the minimum value of the signal and dividing by the range between the maximum and minimum values. 
The result is then multiplied by 100 to scale the normalized signal to the desired range of 0 to 100.
"""


def normalization(signal_):
    max_ = np.max(signal_)
    min_ = np.min(signal_)
    return 100 * (signal_ - min_) / (max_ - min_)


#path = '/content/drive/MyDrive/HCI_project/3-class'
path = '3-class'

"""**Yukari-->Up**

**Asagi-->Down**

**Sag-->Right**

**Sol-->Left**

**Kirp-->Blink**

# Preparing Data

We prepared pair of each signal ( signal.h , signal.v)
for the 5 classes
each one has the directory path of signals
"""

files = os.listdir(os.path.join(path))

blink_pair = []
right_pair = []
left_pair = []
up_pair = []
down_pair = []

for i in files:
    if "asagi" in i:  # dowm
        if i.endswith('h.txt'):

            signal_v = i[:-5] + 'v.txt'
            # print(signal_v)
            if signal_v in files:
                down_pair.append([os.path.join(path, i), os.path.join(path, signal_v)])

    elif "kirp" in i:  # blink
        if i.endswith('h.txt'):

            signal_v = i[:-5] + 'v.txt'
            # print(signal_v)
            if signal_v in files:
                blink_pair.append([os.path.join(path, i), os.path.join(path, signal_v)])


    elif "sag" in i:  # right
        if i.endswith('h.txt'):
            signal_v = i[:-5] + 'v.txt'
            # print(signal_v)
            if signal_v in files:
                right_pair.append([os.path.join(path, i), os.path.join(path, signal_v)])


    elif "sol" in i:  # left
        if i.endswith('h.txt'):
            signal_v = i[:-5] + 'v.txt'
            # print(signal_v)
            if signal_v in files:
                left_pair.append([os.path.join(path, i), os.path.join(path, signal_v)])


    elif "yukari" in i:  # up
        if i.endswith('h.txt'):
            signal_v = i[:-5] + 'v.txt'
            # print(signal_v)
            if signal_v in files:
                up_pair.append([os.path.join(path, i), os.path.join(path, signal_v)])


"""# Feature Extraction"""

import pywt



def waveltes_feature_extraction(signal):
    wavelet = 'db4'  # Wavelet function
    level = 2
    coeffs = pywt.wavedec(signal, wavelet, level=level)
    return coeffs[0]


def Morphological_feature_extraction(signal):
    feature1 = []

    Area_under_curve = integrate.simpson(signal)
    feature1.append(Area_under_curve)

    peaks, _ = find_peaks(signal)

    return peaks


"""For machine learning, auto regressive coefficients can be used as features. For
each EOG signal, the auto regressive coefficients are computed and used to
represent the EOG signal (number of coefficients << signal length)
"""

from statsmodels.tsa.ar_model import AutoReg


def AutoRegrtion(signal):
    model = AutoReg(signal, lags=4)
    model_fit = model.fit()
    print('Coefficient: %s' % model_fit.params)
    # plt.figure(figsize=(12,6))
    # plt.plot(np.arange(0, len(signal)) , signal )
    # plt.xlabel("time")
    # plt.ylabel("amp")

    return model_fit.params


def Read_signal(signal_txt):
    signal = open(signal_txt, "r")
    lines = signal.readlines()

    amp = []
    for i in range(len(lines) - 1):
        L = lines[i]
        amp.append(int(L))

    return amp


data_x = []
data_y = []

def read(pairs):
    for hor, ver in pairs:

        # read signal
        Amp_hor = Read_signal(hor)
        Amp_ver = Read_signal(ver)
        return Amp_hor,Amp_ver


def preprocessing_each_pair(pairs, label):  # it take list of pairs

    for hor, ver in pairs:

        # print("hor = ", hor)
        # print("ver = ", ver)
        # read signal
        if label !=None:
            hor = Read_signal(hor)
            ver = Read_signal(ver)
        else:
            run_data = []

        # normalization
        norm_hor = normalization(hor)
        norm_ver = normalization(ver)
        # filtering

        filtered_hor = Butterworth_filter(hor, low_cutoff=0.5, high_cutoff=20.0, sampling_rate=176, order=2)
        filtered_ver = Butterworth_filter(ver, low_cutoff=0.5, high_cutoff=20.0, sampling_rate=176, order=2)

        # Resampling (down sampling)
        '''NOTE : if you will use AutoRegrtion resample with 50 
                  but if you will use waveltes_feature_extraction resample with 128'''
        resampled_hor = resampling_signal(filtered_hor, 50)
        resampled_ver = resampling_signal(filtered_ver, 50)

        # feature extraction by AutoRegressive
        feature_hor = AutoRegrtion(resampled_hor)
        feature_ver = AutoRegrtion(resampled_ver)

        # feature extraction by Waveletes
        # feature_hor = waveltes_feature_extraction(resampled_hor)
        # feature_ver = waveltes_feature_extraction(resampled_ver)

        # feature_hor = Morphological_feature_extraction(resampled_hor)
        # feature_ver = Morphological_feature_extraction(resampled_ver)

        # print("feature_hor = ", feature_hor)
        # print("feature_ver = ", feature_ver)

        # concatenation

        # feature = feature_hor + feature_ver
        feature = []
        for i in feature_hor:
            feature.append(i)

        for i in feature_ver:
            feature.append(i)

        # print("feature", feature)
        if label !=None:
            data_x.append(feature)
            data_y.append(label)
        else:
            run_data.append(feature)

    if label == None:
        return run_data


preprocessing_each_pair(down_pair, 0)

preprocessing_each_pair(up_pair, 1)

preprocessing_each_pair(right_pair, 2)

preprocessing_each_pair(left_pair, 3)

preprocessing_each_pair(blink_pair, 4)



# print(len(data_x))
#
# print(len(data_y))
if close_train:
    print("Lets Move the circle")
else:
    # Shuffle the data
    x, y = shuffle(data_x, data_y, random_state=42)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

    # Split the training data into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    # _____________________classifiers____________________-

    # Create a Gaussian Naive Bayes classifier
    # clf = GaussianNB()

    # Create a Gradient Boosting classifier
    # clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, random_state=42)

    import joblib

    clf = RandomForestClassifier(n_estimators=100, random_state=42)

    # clf = LogisticRegression()

    # Create an SVM classifier with a linear kernel
    # clf = svm.SVC(kernel='linear')
    # ________________Training__________________________

    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    train_acc = clf.score(X_train, y_train)
    val_acc = clf.score(X_val, y_val)
    test_acc = clf.score(X_test, y_test)
    print("Training accuracy:", train_acc)
    print("Validation accuracy:", val_acc)
    print("Testing accuracy:", test_acc)
    # _________________________________________________________
    accuracy= f"""Training accuracy: {train_acc}\nValidation accuracy: {val_acc}\nTesting accuracy: {test_acc}"""

    joblib.dump(clf, 'model.joblib')
    joblib.dump(accuracy, 'result.text')

    # Create a confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Create a heatmap of the confusion matrix
    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=['Down 0', 'up 1', 'right 2', 'left 3', 'blink 4'],
                yticklabels=['Down 0', 'up 1', 'right 2', 'left 3', 'blink 4'])

    # Add labels and title
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.title('Confusion Matrix')

    # Show the plot
    plt.show()

"""trial 1

test dataset: 10%

train dataset: 90%

classifier : 


GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, random_state=42) , accuracy 80% 

when the way of concatenation by
 "+" > accuracy 70%
 manually > accuracy 80% 

LogisticRegression() , 100%

RandomForestClassifier , 100%

GaussianNB() , 80%

----------------------------
trial 2 

test dataset: 20%

train dataset: 80%

manual concatenation 

classifier :

GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, random_state=42) - accuracy : 85%

RandomForestClassifier(n_estimators=100, random_state=42) , accuracy : 100%

svm.SVC(kernel='linear') , accuracy : 75%

LogisticRegression() , accuracy : 75%

GaussianNB() , accuracy 85%
"""

"""test dataset: 20%

train dataset: 80%

RandomForestClassifier
Auto Regressive 
resampling with 128 or 32 > accurcy 75%

resampling with 64 > accurcy 85%

resampling with 100  > accurcy 65%

resampling with 50 > accurcy 100%

but waveletes 
resampling with 128 > accurcy 100%
resampling with 50 > accurcy 95%
"""
